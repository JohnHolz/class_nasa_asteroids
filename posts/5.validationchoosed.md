# Cross Validationw

Vamos utilizar um k-fold como forma de validção, dividiremos em 10. K-folds funciona separando o dataset em k subsets de dados, e como temos 10 partições agora, podemos treinar em k-1 e testamos em k, e repetiremos esse processo k vezes. Assim teremos métricas mais confiáveis.

    "daí que a universalidade empírica é uma extensão arbitraria de validade"

Sabendo que existem callbacks eu não adicionei pois tive de aguardar pacientemente pelo treinamento dos modelos. Depois de 2 bitcoins minerados pelo tensorflow chegamos aos seguintes resultadossw

# Conclusão

Vemos que as métricas incrivelmente melhoraram, pois em cada interação do k-folds tem mais dados do que utilizamos na escolha dos modelos, o que mostra que fizemos um bom trabalho. Conseguimos um nível de qualidade de modelos muito bom,
otimizamos a nossa métrica de falsos negativos, chegamos em 0 e nosso caso que mais erramos no k-folds erramos pouquissimos dados.

Achei que no final seriamos derrotados pelo asteroide invísivel desfarçado de bonzinho mas conseguimos fazer os modelos acusarem.
